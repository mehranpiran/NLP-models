{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5171957-e9d0-4b64-9dd8-35bb66a5877d",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c6c9dc-bc90-42f3-b72f-f32ef9709979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0382489-58a0-45bf-a211-cf2a4a6cb68f",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0309ae98-4b73-4f62-aded-ab684b467e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL DATA:\n",
      "     Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/ner_dataset.csv\", encoding = \"ISO-8859-1\") \n",
    "print('ORIGINAL DATA:\\n', data.head())\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bf1faf5-5eca-4d64-bd96-7abacf5e2883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        data = np.array([line.strip() for line in file.readlines()])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18c44d0d-97ba-4dc2-9067-4f388c7ee561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sentences = load_data('data/large/train/sentences.txt')\n",
    "train_labels = load_data('data/large/train/labels.txt')\n",
    "\n",
    "val_sentences = load_data('data/large/val/sentences.txt')\n",
    "val_labels = load_data('data/large/val/labels.txt')\n",
    "\n",
    "test_sentences = load_data('data/large/test/sentences.txt')\n",
    "test_labels = load_data('data/large/test/labels.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de12d3cd-f420-4b40-ae24-f15ab902c274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33570,)\n",
      "(7194,)\n",
      "(7194,)\n",
      "\n",
      "  ['Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .'\n",
      " 'Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \" Bush Number One Terrorist \" and \" Stop the Bombings . \"']\n",
      "\n",
      "  ['O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O'\n",
      " 'O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O']\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences.shape)\n",
    "print(val_sentences.shape)\n",
    "print(test_sentences.shape)\n",
    "print(\"\\n \",train_sentences[0:2])\n",
    "print(\"\\n \",train_labels[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f3491-2816-41e7-9682-31893ad2659d",
   "metadata": {},
   "source": [
    "## 3 - Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91477d8f-e15e-4885-8442-e4b5a0db1904",
   "metadata": {},
   "source": [
    "### 3.1 Encoding the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bd9c0-d413-414c-b87b-f4d7452d8f42",
   "metadata": {},
   "source": [
    "We will use tf.keras.layers.TextVectorization to transform the sentences into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e24a2d7-bfac-4e6e-84b0-b8e3bcfaae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentence_vectorizer(sentences):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a TextVectorization layer for sentence tokenization and adapt it to the provided sentences.\n",
    "\n",
    "    Parameters:\n",
    "    sentences (list of str): Sentences for vocabulary adaptation.\n",
    "\n",
    "    Returns:\n",
    "    sentence_vectorizer (tf.keras.layers.TextVectorization): TextVectorization layer for sentence tokenization.\n",
    "    vocab (list of str): Extracted vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define TextVectorization object with the appropriate standardize parameter\n",
    "    sentence_vectorizer = tf.keras.layers.TextVectorization(standardize=None)\n",
    "    # Adapt the sentence vectorization object to the given sentences\n",
    "    sentence_vectorizer.adapt(sentences)\n",
    "    # Get the vocabulary\n",
    "    vocab = sentence_vectorizer.get_vocabulary()\n",
    "    \n",
    "    return sentence_vectorizer, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bc4edbc-d1f0-42f8-99b0-dc7c6f99ea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 4650\n",
      "Head vocab: ['', '[UNK]', 'the', '.', ',', 'in']\n",
      "Sentence: I like learning new NLP models !\n",
      "Sentence vectorized: [ 296  314    1   59    1    1 4649]\n"
     ]
    }
   ],
   "source": [
    "sentence_vectorizer, vocab = get_sentence_vectorizer(train_sentences)\n",
    "print(f\"vocab size: {len(test_vocab)}\")\n",
    "print(f\"Head vocab: {test_vocab[0:6]}\")\n",
    "\n",
    "sentence = \"I like learning new NLP models !\"\n",
    "sentence_vectorized = test_vectorizer(sentence)\n",
    "print(f\"Sentence: {sentence}\\nSentence vectorized: {sentence_vectorized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5289a6-3540-4a06-b3b4-46f97fb1dccd",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Encoding the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48a7c6-4c50-4d9b-94b1-54ed2a466460",
   "metadata": {},
   "source": [
    "Extract all the different tags in a given set of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50af21ec-7f91-4733-bfab-504004770799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O',\n",
       "       'O O O O O O O O O O O O O O O O O O B-per O O O O O O O O O O O',\n",
       "       'O O O O O O O O O O O B-geo I-geo O', ...,\n",
       "       'B-per I-per O O O B-tim O O O O O O O O O O',\n",
       "       'B-gpe O B-per I-per O O O O O B-org I-org I-org O O O O',\n",
       "       'O O O O O O B-geo O O O O O O O O O O O O O O O O'], dtype='<U287')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccf624f0-c14f-4957-bf59-20382eb77834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per', 'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org', 'I-per', 'I-tim', 'O']\n"
     ]
    }
   ],
   "source": [
    "def get_tags(labels):\n",
    "    tag_set = set() # Define an empty set\n",
    "    for el in labels:\n",
    "        for tag in el.split(\" \"):\n",
    "            tag_set.add(tag)\n",
    "    tag_list = list(tag_set) \n",
    "    tag_list.sort()\n",
    "    return tag_list\n",
    "\n",
    "tags = get_tags(train_labels)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8eaebe-8e54-416f-8a15-c56dffffc393",
   "metadata": {},
   "source": [
    "Now you will need to generate a **tag map**, i.e., a mapping between the tags and **positive** integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdeef3aa-634a-43cc-9a49-792e22bd6f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n"
     ]
    }
   ],
   "source": [
    "def make_tag_map(tags):\n",
    "    tag_map = {}\n",
    "    for i,tag in enumerate(tags):\n",
    "        tag_map[tag] = i \n",
    "    return tag_map\n",
    "\n",
    "tag_map = make_tag_map(tags)\n",
    "print(tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beed0c3-18fe-4aee-8e06-0ef837f59937",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Building the label vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793a3a2d-7a9c-489c-97a8-b50d0a1b902c",
   "metadata": {},
   "source": [
    "In this section, we will pad the labels. TextVectorization already padded the sentences, so you must ensure that the labels are properly padded as well.         \n",
    "We will pad the vectorized labels with the value -1. You will not use 0 to simplify loss masking and evaluation in further steps. This is because to properly classify one token, a log softmax transformation will be performed and the index with greater value will be the index label. Since index starts at 0, it is better to keep the label 0 as a valid index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73b38a-a4e5-4f48-9f61-4eeac7aae33a",
   "metadata": {},
   "source": [
    "Tensorflow provides the function tf.keras.utils.pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03f5df77-4906-4824-a27d-1e815269c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_vectorizer(labels, tag_map):\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert list of label strings to padded label IDs using a tag mapping.\n",
    "\n",
    "    Parameters:\n",
    "    labels (list of str): List of label strings.\n",
    "    tag_map (dict): Dictionary mapping tags to IDs.\n",
    "    Returns:\n",
    "    label_ids (numpy.ndarray): Padded array of label IDs.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    label_ids = [] # It can't be a numpy array yet, since each sentence has a different size\n",
    "\n",
    "    # Each element in labels is a string of tags so for each of them:\n",
    "    for element in labels:\n",
    "        # Split it into single tokens. You may use .split function for strings. Be aware to split it by a blank space!\n",
    "        tokens = element.split(' ')\n",
    "\n",
    "        # Use the dictionaty tag_map passed as an argument to the label_vectorizer function\n",
    "        # to make the correspondence between tags and numbers. \n",
    "        element_ids = [tag_map[tag] for tag in tokens]\n",
    "\n",
    "\n",
    "        # Append the found ids to corresponding to the current element to label_ids list\n",
    "        label_ids.append(element_ids)\n",
    "        \n",
    "    # Pad the elements\n",
    "    label_ids = tf.keras.utils.pad_sequences(sequences=label_ids, padding='post', value=-1)\n",
    "\n",
    "    return label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b0a51bb-accb-429b-8d19-6bd9a07b18b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The party is divided over Britain 's participation in the Iraq conflict and the continued deployment of 8,500 British troops in that country .\n",
      "Labels: O O O O O B-gpe O O O O B-geo O O O O O O O B-gpe O O O O O\n",
      "Vectorized labels: [[16 16 16 16 16  3 16 16 16 16  2 16 16 16 16 16 16 16  3 16 16 16 16 16]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_sentences[5]}\")\n",
    "print(f\"Labels: {train_labels[5]}\")\n",
    "print(f\"Vectorized labels: {label_vectorizer([train_labels[5]], tag_map)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a5a7f-d3de-457b-9525-22e3f4a9ecbc",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 Building the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e51ea811-cd6a-49eb-868c-4892dc760a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(sentences, labels, sentence_vectorizer, tag_map):\n",
    "    sentences_ids = sentence_vectorizer(sentences)\n",
    "    labels_ids = label_vectorizer(labels, tag_map)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sentences_ids, labels_ids))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "636a2eaf-d586-4b94-bc83-2d989110ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = generate_dataset(train_sentences,train_labels, sentence_vectorizer, tag_map)\n",
    "val_dataset = generate_dataset(val_sentences,val_labels,  sentence_vectorizer, tag_map)\n",
    "test_dataset = generate_dataset(test_sentences, test_labels,  sentence_vectorizer, tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5faad263-c003-4d4d-9b23-bee55c0b8a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TensorSliceDataset element_spec=(TensorSpec(shape=(104,), dtype=tf.int64, name=None), TensorSpec(shape=(104,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "#Use take(n) to take the first n elements\n",
    "#Use skip(n) to skip the first n elements\n",
    "#print('An example of the first sentence is\\n', next(iter(train_dataset))[0].numpy())\n",
    "#print('An example of its corresponding label is\\n', next(iter(train_dataset))[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f1166b9-d7a9-4aa6-a8ef-810f7e7839e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(104,), dtype=int64, numpy=\n",
      "array([1046,    6, 1121,   18, 1832,  232,  543,    7,  528,    2,  158,\n",
      "          5,   60,    9,  648,    2,  922,    6,  192,   87,   22,   16,\n",
      "         54,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0])>, <tf.Tensor: shape=(104,), dtype=int32, numpy=\n",
      "array([16, 16, 16, 16, 16, 16,  2, 16, 16, 16, 16, 16,  2, 16, 16, 16, 16,\n",
      "       16,  3, 16, 16, 16, 16, 16, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1], dtype=int32)>)\n",
      "(<tf.Tensor: shape=(104,), dtype=int64, numpy=\n",
      "array([10265,     6,   156,    38,     5,     2,   492,  1083,     2,\n",
      "         653,    53,   606,  6039,    17,   304,  3163,    26,    30,\n",
      "         118, 25159,   805, 11825,    30,     9,    30, 15268,     2,\n",
      "       10354,     3,    30,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0])>, <tf.Tensor: shape=(104,), dtype=int32, numpy=\n",
      "array([16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16,  6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1], dtype=int32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 05:39:07.336593: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [33570,104]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "for element in train_dataset.take(2):\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6a486c1-ef56-44d3-9b3a-6813127e1cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of a sentence is\n",
      " [1046    6 1121   18 1832  232  543    7  528    2  158    5   60    9\n",
      "  648    2  922    6  192   87   22   16   54    3    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "An example of its corresponding label is\n",
      " [16 16 16 16 16 16  2 16 16 16 16 16  2 16 16 16 16 16  3 16 16 16 16 16\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "An example of a sentence is\n",
      " [10265     6   156    38     5     2   492  1083     2   653    53   606\n",
      "  6039    17   304  3163    26    30   118 25159   805 11825    30     9\n",
      "    30 15268     2 10354     3    30     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "An example of its corresponding label is\n",
      " [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16  6 16 16 16 16 16\n",
      " 16 16 16 16 16 16 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Create an iterator for the dataset\n",
    "dataset_iter = iter(train_dataset)\n",
    "\n",
    "# Print the first 2 examples\n",
    "for _ in range(2):\n",
    "    example_batch = next(dataset_iter)\n",
    "    sentence, label = example_batch\n",
    "    print('An example of a sentence is\\n', sentence.numpy())\n",
    "    print('An example of its corresponding label is\\n', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371c8e81-6df1-4cb2-8b0d-32ddd801b847",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 5 Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed60e38b-3877-4600-a360-46c575addef0",
   "metadata": {},
   "source": [
    "Inputs are sentences represented as tensors that are fed to a model with:\n",
    "\n",
    "An Embedding layer           \n",
    "A LSTM layer           \n",
    "A Dense layer             \n",
    "A log softmax layer             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4b68c83-a77c-4957-bd88-4c7b1b2b726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def NER(len_tags, vocab_size, embedding_dim = 50):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a Named Entity Recognition (NER) model.\n",
    "\n",
    "    Parameters:\n",
    "    len_tags (int): Number of NER tags (output classes).\n",
    "    vocab_size (int): Vocabulary size.\n",
    "    embedding_dim (int, optional): Dimension of embedding and LSTM layers (default is 50).\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): NER model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential(name = 'sequential') \n",
    "    # Add the tf.keras.layers.Embedding layer. Do not forget to mask out the zeros!\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, mask_zero=False))\n",
    "\n",
    "    # Masking layer to handle -1 padding in labels\n",
    "    model.add(tf.keras.layers.Masking(mask_value=-1))\n",
    "    \n",
    "    # Add the LSTM layer. Make sure you are passing the right dimension\n",
    "    # and returning every output for the tf.keras.layers.LSTM layer and not the very last one.\n",
    "    model.add(tf.keras.layers.LSTM(units=embedding_dim, return_sequences=True))\n",
    "    # Add the final tf.keras.layers.Dense with the appropriate activation function. Remember you must pass the activation function itself ant not its call!\n",
    "    # You must use tf.nn.log_softmax instead of tf.nn.log_softmax().\n",
    "    model.add(tf.keras.layers.Dense(units=len_tags, activation=tf.nn.log_softmax))\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e44fa-de68-4027-9e4b-7b76ca03a3ae",
   "metadata": {},
   "source": [
    "Before training the model, you need to create your own function to compute the accuracy. Tensorflow has built-in accuracy metrics but you cannot pass values to be ignored. This will impact the calculations, since you must remove the padded values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1571c17d-10ed-4c9f-b184-37c133f3c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_loss(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the masked sparse categorical cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tensor): True labels.\n",
    "    y_pred (tensor): Predicted logits.\n",
    "    \n",
    "    Returns:\n",
    "    loss (tensor): Calculated loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True , ignore_class = -1)\n",
    "\n",
    "    loss = loss_fn(y_true,y_pred)\n",
    "    \n",
    "    return  loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e246352-a895-4514-a1bb-d5d6fc474c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.1242604, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "true_labels = [0,1,2,0]\n",
    "predicted_logits = [[-2.3,-0.51,-1.20] , [-1.61,-0.36,-2.30], [-2.30, -0.69,-0.92], [-0.92,-0.92,-1.61]]\n",
    "print(masked_loss(true_labels, predicted_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f6cc93b-8633-4f2c-9603-63dc37c33cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate masked accuracy for predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tensor): True labels.\n",
    "    y_pred (tensor): Predicted logits.\n",
    "\n",
    "    Returns:\n",
    "    accuracy (tensor): Masked accuracy.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss for each item in the batch.\n",
    "    # You must always cast the tensors to the same type in order to use them in training. Since you will make divisions, it is safe to use tf.float32 data type.\n",
    "    y_true = tf.cast(y_true, tf.float32) \n",
    "    # Create the mask, i.e., the values that will be ignored\n",
    "    mask = tf.not_equal(y_true , -1)\n",
    "    mask = tf.cast(mask, tf.float32) \n",
    "    # Perform argmax to get the predicted values\n",
    "    y_pred_class = tf.math.argmax(y_pred, axis=-1)\n",
    "    y_pred_class = tf.cast(y_pred_class, tf.float32) \n",
    "    # Compare the true values with the predicted ones\n",
    "    matches_true_pred  = tf.equal(y_pred_class, y_true)\n",
    "    matches_true_pred = tf.cast(matches_true_pred , tf.float32) \n",
    "    # Multiply the acc tensor with the masks\n",
    "    matches_true_pred *= mask\n",
    "    # Compute masked accuracy (quotient between the total matches and the total valid values, i.e., the amount of non-masked values)\n",
    "    masked_acc = tf.reduce_sum(matches_true_pred) / tf.reduce_sum(mask)\n",
    "    \n",
    "    return masked_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31d30cab-a823-483d-ada8-c75cdae94b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "true_labels = [0,1,2,0]\n",
    "predicted_logits = [[0.1,0.6,0.3] , [0.2,0.7,0.1], [0.1, 0.5,0.4], [0.4,0.4,0.2]]\n",
    "print(masked_accuracy(true_labels, predicted_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc3d6784-05be-4297-93b4-47fd83f82ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29847\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(len(tag_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce4da396-ec21-464b-b01b-8c82d502cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 50)          1492400   \n",
      "                                                                 \n",
      " masking_1 (Masking)         (None, None, 50)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 50)          20200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 17)          867       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,513,467\n",
      "Trainable params: 1,513,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = NER(len(tag_map), len(vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "483a0b45-6ba8-4689-ad2b-cbf2c6bbc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01), \n",
    "              loss = masked_loss,\n",
    "               metrics = [masked_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a1893-845b-4c42-b0da-c11bf3d59ce0",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6bb60a04-9993-42a0-aab9-9a534b6c9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 09:06:34.598839: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [33570,104]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525/525 [==============================] - ETA: 0s - loss: 0.2687 - masked_accuracy: 0.9305"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 09:07:47.650277: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [7194,73]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525/525 [==============================] - 77s 140ms/step - loss: 0.2687 - masked_accuracy: 0.9305 - val_loss: 0.1397 - val_masked_accuracy: 0.9584\n",
      "Epoch 2/10\n",
      "525/525 [==============================] - 74s 140ms/step - loss: 0.1099 - masked_accuracy: 0.9657 - val_loss: 0.1361 - val_masked_accuracy: 0.9586\n",
      "Epoch 3/10\n",
      "525/525 [==============================] - 74s 140ms/step - loss: 0.0889 - masked_accuracy: 0.9711 - val_loss: 0.1393 - val_masked_accuracy: 0.9584\n",
      "Epoch 4/10\n",
      "525/525 [==============================] - 69s 132ms/step - loss: 0.0778 - masked_accuracy: 0.9743 - val_loss: 0.1451 - val_masked_accuracy: 0.9587\n",
      "Epoch 5/10\n",
      "525/525 [==============================] - 75s 142ms/step - loss: 0.0697 - masked_accuracy: 0.9767 - val_loss: 0.1547 - val_masked_accuracy: 0.9570\n",
      "Epoch 6/10\n",
      "525/525 [==============================] - 68s 130ms/step - loss: 0.0636 - masked_accuracy: 0.9787 - val_loss: 0.1626 - val_masked_accuracy: 0.9556\n",
      "Epoch 7/10\n",
      "525/525 [==============================] - 67s 128ms/step - loss: 0.0589 - masked_accuracy: 0.9798 - val_loss: 0.1743 - val_masked_accuracy: 0.9536\n",
      "Epoch 8/10\n",
      "525/525 [==============================] - 70s 134ms/step - loss: 0.0550 - masked_accuracy: 0.9811 - val_loss: 0.1819 - val_masked_accuracy: 0.9523\n",
      "Epoch 9/10\n",
      "525/525 [==============================] - 73s 140ms/step - loss: 0.0520 - masked_accuracy: 0.9822 - val_loss: 0.1837 - val_masked_accuracy: 0.9535\n",
      "Epoch 10/10\n",
      "525/525 [==============================] - 77s 147ms/step - loss: 0.0489 - masked_accuracy: 0.9831 - val_loss: 0.1927 - val_masked_accuracy: 0.9534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1170638f50>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(33) ## Setting again a random seed to ensure reproducibility\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model.fit(train_dataset.batch(BATCH_SIZE),\n",
    "          validation_data = val_dataset.batch(BATCH_SIZE),\n",
    "          shuffle=True,\n",
    "          epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b903e9-8ede-4f88-b0f4-26606011e5c8",
   "metadata": {},
   "source": [
    "### Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "53898c0d-2ec3-4d0e-8797-372f400be1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 5s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "test_sentences_id = sentence_vectorizer(test_sentences)\n",
    "test_labels_id = label_vectorizer(test_labels,tag_map)\n",
    "y_true = test_labels_id \n",
    "y_pred = model.predict(test_sentences_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "67c2c6ba-60fe-4dfa-8035-37e0e206d6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's accuracy in test set is: 0.9537\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model's accuracy in test set is: {masked_accuracy(y_true,y_pred).numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdaf730-68ee-49c7-8597-315578409fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76cd56-840d-40d8-920d-69f47bfc4b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc65b5-09fe-4e92-8af2-43e476b1ecd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217c85e-d5ca-4a7c-b715-26ba086b1fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
