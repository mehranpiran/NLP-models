{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753075b7-8250-4e85-9c2d-f20c9a2dfeea",
   "metadata": {},
   "source": [
    "# Word prediction and sequence generation for shakespeare play with LSTM and GRU. Mehran Piran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "032471c2-a333-4b46-ba8e-0cf669e2a3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-14 18:03:46.729764: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "\n",
    "import shutil\n",
    "import numpy as np\n",
    "import random as  rnd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a9c4711-2edd-47b1-ac80-d57af6868760",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/mnt/market/anclab-rstudio-server/home/mpir0002/NLP_labs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ce491-0959-4f8f-aace-17aac89a7d2c",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ca44c144-b30d-494c-8ba3-40c5581d1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dirname = '/mnt/market/anclab-rstudio-server/home/mpir0002/NLP_labs'\n",
    "filename = 'shakespeare_data.txt'\n",
    "file_path = os.path.join(dirname, filename)\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read all lines into a list\n",
    "    lines = file.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2d0c63a8-068b-486c-a32a-eee7ff8b7193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'FROM off a hill whose concave womb reworded\\n',\n",
       " 'A plaintful story from a sistering vale,\\n',\n",
       " 'My spirits to attend this double voice accorded,\\n',\n",
       " 'And down I laid to list the sad-tuned tale;\\n',\n",
       " 'Ere long espied a fickle maid full pale,\\n',\n",
       " 'Tearing of papers, breaking rings a-twain,\\n']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5f8b7e16-2b4c-439c-8f9e-edcad0f2f02a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FROM off a hill whose concave womb reworded\n",
      "\n",
      "A plaintful story from a sistering vale,\n",
      "\n",
      "My spirits to attend this double voice accorded,\n",
      "\n",
      "And down I laid to list the sad-tuned tale;\n",
      "\n",
      "Ere long espied a fickle maid full pale,\n",
      "\n",
      "Tearing of papers, breaking rings a-twain,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(lines[1:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "715e906e-a3ad-4f31-8b57-0946adce226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lines = [] # storing all the lines in a variable. \n",
    "\n",
    "counter = 0\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:        \n",
    "        # remove leading and trailing whitespace\n",
    "        pure_line = line.strip().lower()\n",
    "\n",
    "        # if pure_line is not the empty string,\n",
    "        if pure_line:\n",
    "            # append it to the list\n",
    "            lines.append(pure_line)\n",
    "            \n",
    "n_lines = len(lines)\n",
    "print(f\"Number of lines: {n_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0996dfec-599f-4a6e-879c-91b8594f7a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['from off a hill whose concave womb reworded',\n",
       " 'a plaintful story from a sistering vale,',\n",
       " 'my spirits to attend this double voice accorded,',\n",
       " 'and down i laid to list the sad-tuned tale;',\n",
       " 'ere long espied a fickle maid full pale,',\n",
       " 'tearing of papers, breaking rings a-twain,',\n",
       " \"storming her world with sorrow's wind and rain.\",\n",
       " 'upon her head a platted hive of straw,',\n",
       " 'which fortified her visage from the sun,']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = lines[1:10]\n",
    "print(len(l1))\n",
    "print(type(l1))\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "094e0618-e761-4c58-aa90-6acc958a290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"from off a hill whose concave womb reworded\\na plaintful story from a sistering vale,\\nmy spirits to attend this double voice accorded,\\nand down i laid to list the sad-tuned tale;\\nere long espied a fickle maid full pale,\\ntearing of papers, breaking rings a-twain,\\nstorming her world with sorrow's wind and rain.\\nupon her head a platted hive of straw,\\nwhich fortified her visage from the sun,\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = \"\\n\".join(lines[1:10])\n",
    "print(len(l2))\n",
    "print(type(l2))\n",
    "l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856ca0c-f57b-4bac-b540-beb8d62c5849",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 2) Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63222d-2095-41f4-80f8-77dc4abbe4a6",
   "metadata": {},
   "source": [
    "Because we are performing sequence generation, A vocabulary of letters are created, not words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d80e6732-33fa-4d11-808c-eb5ec8cb7079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '', '\\t', '\\n', ' ', '!', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|']\n"
     ]
    }
   ],
   "source": [
    "# 2) Create vocabulary  \n",
    "text = \"\\n\".join(lines)\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "vocab.insert(0,\"[UNK]\") # Add a special character for any unknown\n",
    "vocab.insert(1,\"\") # Add the empty character for padding.\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3dcad5cc-fe8d-4091-8c88-ff5dddd13441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "33\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(vocab.index('a'))\n",
    "print(vocab.index('e'))\n",
    "print(vocab.index('i'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93144208-b6aa-462b-8900-54bf8132f8dc",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 3) Convert each line to a tensor and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0802cebe-9601-4c79-b258-f204231f0fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'H' b'e' b'l' b'l' b'o' b' ' b'w' b'o' b'r' b'l' b'd' b'!'], shape=(12,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "line = \"Hello world!\"\n",
    "chars = tf.strings.unicode_split(line, input_encoding='UTF-8')\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9eb5ca6-c52c-41b9-97f5-b0acb8d45e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0 33 40 40 43  4 51 43 46 40 32  5], shape=(12,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "ids = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)(chars)\n",
    "print(ids)\n",
    "#  By setting mask_token=None, you are indicating that no token in the vocabulary should be treated as a mask token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a72dccb-7c17-4f7e-84cf-dbabe4d909a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line, vocab):\n",
    "    \"\"\"\n",
    "    Converts a line of text into a tensor of integer values representing characters.\n",
    "\n",
    "    Args:\n",
    "        line (str): A single line of text.\n",
    "        vocab (list): A list containing the vocabulary of unique characters.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor(dtype=int64): A tensor containing integers (unicode values) corresponding to the characters in the `line`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the input line into individual characters\n",
    "    chars = list(line)\n",
    "    # Map characters to their respective integer values using StringLookup\n",
    "    ids = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)(chars)\n",
    "    \n",
    "\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59012b25-a974-4dba-94f9-344bcd47726a",
   "metadata": {},
   "source": [
    "You will also need a function that produces text given a numeric tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6621e22e-7add-40ce-a77a-4ed8d908de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_from_ids(ids, vocab):\n",
    "    \n",
    "    \"\"\"\n",
    "    Converts a tensor of integer values into human-readable text.\n",
    "\n",
    "    Args:\n",
    "        ids (tf.Tensor): A tensor containing integer values (unicode IDs).\n",
    "        vocab (list): A list containing the vocabulary of unique characters.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the characters in human-readable format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the StringLookup layer to map integer IDs back to characters\n",
    "    chars_from_ids = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True, mask_token=None)\n",
    "    \n",
    "    # Use the layer to decode the tensor of IDs into human-readable text\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c4cbe-6105-4d8d-9d12-c96ce7826f3d",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 3.1) Prepare your data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "335b7ae4-5d9f-43fc-bb9a-85c1bb35e0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training lines: 124097\n",
      "Number of validation lines: 1000\n"
     ]
    }
   ],
   "source": [
    "train_lines = lines[:-1000] # Leave the rest for training\n",
    "eval_lines = lines[-1000:] # Create a holdout validation set\n",
    "\n",
    "print(f\"Number of training lines: {len(train_lines)}\")\n",
    "print(f\"Number of validation lines: {len(eval_lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efc1ad-64e0-4502-9343-a13b69876fce",
   "metadata": {},
   "source": [
    "### Create input and output for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e3ecc-eb3c-4f17-8f04-98906d719db1",
   "metadata": {},
   "source": [
    "In many sequence modeling tasks (such as language modeling or sequence prediction), you often need to predict the next element in the sequence given the previous elements. This is why you might use seq_length + 1 instead of just seq_length.\n",
    "\n",
    "You have to predict the next character in a sequence. The following function creates 2 tensors, each with a length of seq_length out of the input sequence of lenght seq_length + 1. The first one contains the first seq_length elements and the second one contains the last seq_length elements. For example, if you split the sequence ['H', 'e', 'l', 'l', 'o'], you will obtain the sequences ['H', 'e', 'l', 'l'] and ['e', 'l', 'l', 'o'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9378e63e-2197-4936-ba33-d419cdf55eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    \"\"\"\n",
    "    Splits the input sequence into two sequences, where one is shifted by one position.\n",
    "\n",
    "    Args:\n",
    "        sequence (tf.Tensor or list): A list of characters or a tensor.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor, tf.Tensor: Two tensors representing the input and output sequences for the model.\n",
    "    \"\"\"\n",
    "    # Create the input sequence by excluding the last character\n",
    "    input_text = list(sequence[:-1])\n",
    "    # Create the target sequence by excluding the first character\n",
    "    target_text = list(sequence[1:])\n",
    "\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1232773f-a5bb-4537-8c91-fadc221b84dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(\"Tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423ed2bc-759a-43dd-b350-f9a86b85f309",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 3.2) Data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e0319-76a6-49a6-900f-b326f43a0cd2",
   "metadata": {},
   "source": [
    "Create a TensorFlow DataSet from your numeric tensors using tf.data.Dataset.from_tensor_slices() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8cb97e3b-f507-4856-b5ee-9194bf36863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    \"\"\"\n",
    "    Splits the input sequence into two sequences, where one is shifted by one position.\n",
    "\n",
    "    Args:\n",
    "        sequence (tf.Tensor or list): A list of characters or a tensor.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor, tf.Tensor: Two tensors representing the input and output sequences for the model.\n",
    "    \"\"\"\n",
    "    # Create the input sequence by excluding the last character\n",
    "    input_text = sequence[:-1]\n",
    "    # Create the target sequence by excluding the first character\n",
    "    target_text = sequence[1:]\n",
    "\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7deae778-a414-4ccc-a898-dc195fe07191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tensorflo', 'ensorflow')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(\"Tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3418abf5-b613-4743-8a72-30360433a18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "21dd69c7-109e-4a7b-a455-b0a2f836922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"a lover's complaint\", 'from off a hill whose concave womb reworded', 'a plaintful story from a sistering vale,', 'my spirits to attend this double voice accorded,', 'and down i laid to list the sad-tuned tale;']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"a lover's complaint\\nfrom off a hill whose concave womb reworded\\na plaintful story from a sistering vale,\\nmy spirits to attend this double voice accorded,\\nand down i laid to list the sad-tuned tale;\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = train_lines[0:5]\n",
    "print(lines)\n",
    "single_line_data = \"\\n\".join(lines)\n",
    "single_line_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "705a326e-5d52-4b5f-85a3-df7d8502d6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(197,), dtype=int64, numpy=\n",
       "array([29,  4, 40, 43, 50, 33, 46,  8, 47,  4, 31, 43, 41, 44, 40, 29, 37,\n",
       "       42, 48,  3, 34, 46, 43, 41,  4, 43, 34, 34,  4, 29,  4, 36, 37, 40,\n",
       "       40,  4, 51, 36, 43, 47, 33,  4, 31, 43, 42, 31, 29, 50, 33,  4, 51,\n",
       "       43, 41, 30,  4, 46, 33, 51, 43, 46, 32, 33, 32,  3, 29,  4, 44, 40,\n",
       "       29, 37, 42, 48, 34, 49, 40,  4, 47, 48, 43, 46, 53,  4, 34, 46, 43,\n",
       "       41,  4, 29,  4, 47, 37, 47, 48, 33, 46, 37, 42, 35,  4, 50, 29, 40,\n",
       "       33, 11,  3, 41, 53,  4, 47, 44, 37, 46, 37, 48, 47,  4, 48, 43,  4,\n",
       "       29, 48, 48, 33, 42, 32,  4, 48, 36, 37, 47,  4, 32, 43, 49, 30, 40,\n",
       "       33,  4, 50, 43, 37, 31, 33,  4, 29, 31, 31, 43, 46, 32, 33, 32, 11,\n",
       "        3, 29, 42, 32,  4, 32, 43, 51, 42,  4, 37,  4, 40, 29, 37, 32,  4,\n",
       "       48, 43,  4, 40, 37, 47, 48,  4, 48, 36, 33,  4, 47, 29, 32, 12, 48,\n",
       "       49, 42, 33, 32,  4, 48, 29, 40, 33, 25])>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = line_to_tensor(single_line_data, vocab)\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8162b8ab-fe32-44c3-bb48-142354ae9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "BUFFER_SIZE = 1000\n",
    "batch_size = 64\n",
    "\n",
    "lines = train_lines[0:1000]\n",
    "single_line_data = \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f26d1513-386a-4ff2-8f3e-4fbb2cd8592f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "ids_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0cd2562d-62f7-47d1-b5dc-8dd7250807db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=TensorSpec(shape=(21,), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "514cb216-fa7e-435b-bf19-61cd75594204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=(TensorSpec(shape=(20,), dtype=tf.int64, name=None), TensorSpec(shape=(20,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_xy = data_generator.map(split_input_target)\n",
    "dataset_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7023423a-5a44-4ca5-9935-a951da1b220f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 20), dtype=tf.int64, name=None), TensorSpec(shape=(None, 20), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = (dataset_xy.shuffle(BUFFER_SIZE).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE))  \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9f5b6a49-9071-45ba-bc89-828062115147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_batch_dataset(lines, vocab, seq_length=100, batch_size=64):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a batch dataset from a list of text lines.\n",
    "\n",
    "    Args:\n",
    "        lines (list): A list of strings with the input data, one line per row.\n",
    "        vocab (list): A list containing the vocabulary.\n",
    "        seq_length (int): This parameter specifies the length of each sequence sample that will be used for training. \n",
    "        batch_size (int): The batch size. It determines how many sequence samples are processed together in parallel during each training iteration\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: A batch dataset generator.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Buffer size to shuffle the dataset\n",
    "    # (TF data is designed to work with possibly infinite sequences,\n",
    "    # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "    # it maintains a buffer in which it shuffles elements).\n",
    "    BUFFER_SIZE = 10000\n",
    "    \n",
    "    # For simplicity, just join all lines into a single line\n",
    "    single_line_data  = \"\\n\".join(lines)\n",
    "\n",
    "    # Convert your data into a tensor using the given vocab\n",
    "    all_ids = line_to_tensor(single_line_data, vocab)\n",
    "    # Create a TensorFlow dataset from the data tensor\n",
    "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "    # Create a batch dataset\n",
    "    data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True) # drop_remainder=True means that if the total number of elements in the dataset is not perfectly divisible by the batch size, the remaining elements that do not form a full batch will be discarded.\n",
    "    \n",
    "    dataset_xy = data_generator.map(split_input_target) # Applies the split_input_target function to each batch. This function typically splits each batch into input and target sequences.\n",
    "    \n",
    "    # Assemble the final dataset with shuffling, batching, and prefetching\n",
    "    dataset = (                                   \n",
    "        dataset_xy                                \n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)  \n",
    "        )            \n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c72509-2b4b-4248-937d-f434f0c07fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ids_dataset.batch(seq_length + 1, drop_remainder=True):\n",
    "# This operation batches the dataset into sequences of length seq_length + 1. Each batch produced by this method consists of seq_length + 1 elements. This approach is typically used when preparing data for sequence prediction tasks, where each batch includes both input sequences (first seq_length elements) and target sequences (next element).\n",
    "\n",
    "# dataset.batch(batch_size):\n",
    "# After mapping split_input_target to create (inputs, targets) pairs from each batch, dataset.batch(batch_size) then batches these pairs into batches of size batch_size. This batching step ensures that during training, batch_size number of (inputs, targets) pairs are processed simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d9108-5ea2-40db-8770-fcfd78fb1736",
   "metadata": {},
   "source": [
    "### Test your data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1e975428-6085-406c-8dd6-b08bffa87b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 16), dtype=tf.int64, name=None), TensorSpec(shape=(None, 16), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = create_batch_dataset(train_lines[0:10], vocab, seq_length=16, batch_size=2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "621e65b3-1207-4152-8a94-6166fea30973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'8bc6'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_from_ids([22,30,31,20] , vocab).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "357471ee-5ab9-400d-8838-42539a706cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"sorrow's wind an\"\n",
      "b\"orrow's wind and\"\n",
      "\n",
      " b'attend this doub'\n",
      "b'ttend this doubl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 18:40:44.397278: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [409]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-07-15 18:40:44.397611: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [409]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(text_from_ids(input_example[0], vocab).numpy())\n",
    "    print(text_from_ids(target_example[0], vocab).numpy())\n",
    "    \n",
    "    print(\"\\n\", text_from_ids(input_example[1], vocab).numpy())\n",
    "    print(text_from_ids(target_example[1], vocab).numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a7a55a62-29b1-44e7-830f-1157d75e17fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0, '!': 1, 'l': 2, ' ': 3, 'd': 4, 'r': 5, 'w': 6, 'h': 7, 'o': 8}\n",
      "{0: 'e', 1: '!', 2: 'l', 3: ' ', 4: 'd', 5: 'r', 6: 'w', 7: 'h', 8: 'o'}\n",
      "\n",
      " {'[UNK]': 0, '': 1, '\\t': 2, '\\n': 3, ' ': 4, '!': 5, '$': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, ',': 11, '-': 12, '.': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '[': 27, ']': 28, 'a': 29, 'b': 30, 'c': 31, 'd': 32, 'e': 33, 'f': 34, 'g': 35, 'h': 36, 'i': 37, 'j': 38, 'k': 39, 'l': 40, 'm': 41, 'n': 42, 'o': 43, 'p': 44, 'q': 45, 'r': 46, 's': 47, 't': 48, 'u': 49, 'v': 50, 'w': 51, 'x': 52, 'y': 53, 'z': 54, '|': 55}\n",
      "\n",
      " {0: '[UNK]', 1: '', 2: '\\t', 3: '\\n', 4: ' ', 5: '!', 6: '$', 7: '&', 8: \"'\", 9: '(', 10: ')', 11: ',', 12: '-', 13: '.', 14: '0', 15: '1', 16: '2', 17: '3', 18: '4', 19: '5', 20: '6', 21: '7', 22: '8', 23: '9', 24: ':', 25: ';', 26: '?', 27: '[', 28: ']', 29: 'a', 30: 'b', 31: 'c', 32: 'd', 33: 'e', 34: 'f', 35: 'g', 36: 'h', 37: 'i', 38: 'j', 39: 'k', 40: 'l', 41: 'm', 42: 'n', 43: 'o', 44: 'p', 45: 'q', 46: 'r', 47: 's', 48: 't', 49: 'u', 50: 'v', 51: 'w', 52: 'x', 53: 'y', 54: 'z', 55: '|'}\n",
      "\n",
      " [36, 33, 40, 40, 43, 4, 51, 43, 46, 40, 32, 5]\n",
      "\n",
      " <_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
      "\n",
      " <_BatchDataset element_spec=TensorSpec(shape=(6,), dtype=tf.int32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "line = \"Hello world!\"\n",
    "line = line.lower()\n",
    "\n",
    "char_to_id = {c: i for i, c in enumerate(list(set(line)))}\n",
    "id_to_char = {i: c for i, c in enumerate(list(set(line)))}\n",
    "print(char_to_id)\n",
    "print(id_to_char)\n",
    "char_to_id = {c: i for i, c in enumerate(vocab)}\n",
    "id_to_char = {i: c for i, c in enumerate(vocab)}\n",
    "print(\"\\n\",char_to_id)\n",
    "print(\"\\n\",id_to_char)\n",
    "all_ids = [char_to_id[c] for c in line]\n",
    "print(\"\\n\",all_ids)\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "print(\"\\n\",ids_dataset)\n",
    "\n",
    "seq_length = 5\n",
    "data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "print(\"\\n\",data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a28b41-cdd8-488a-b4a3-659070190de4",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 3.3) Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9218239b-7ea0-4266-9a25-1657877fdfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100), dtype=tf.int64, name=None), TensorSpec(shape=(None, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = create_batch_dataset(train_lines, vocab, seq_length=100, batch_size=BATCH_SIZE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6926c063-395a-4f0c-a294-2d29eb508b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.take(1000)) # Dataset contains 791 batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82fc6f-e7da-43df-9c3c-fad176f0293d",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 4) Constructing the LSTM and GRU language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c6ba78f3-94c3-4501-8755-610a5825d82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "97bf4b6e-1d36-45fe-a971-0183885f2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMLM(tf.keras.Model):\n",
    "    \n",
    "    \"\"\"\n",
    "    A LSTM-based language model that maps from a tensor of tokens to activations over a vocabulary.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
    "        embedding_dim (int, optional): Depth of embedding. Defaults to 256.\n",
    "        rnn_units (int, optional): Number of units in the LSTM cell. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A GRULM language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=256, embedding_dim=256, rnn_units=128):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        \n",
    "        # Create an embedding layer to map token indices to embedding vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # Define a GRU (Gated Recurrent Unit) layer for sequence modeling\n",
    "        self.lstm = tf.keras.layers.LSTM(units=rnn_units, return_sequences=True, return_state=True) # return_state=True, the GRU layer will return the last hidden state in addition to the sequence of outputs\n",
    "        # Apply a dense layer with log-softmax activation to predict next tokens\n",
    "        self.dense = tf.keras.layers.Dense(units=vocab_size, activation=tf.nn.log_softmax)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        # Map input tokens to embedding vectors\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            # Get initial state from the LSTM layer\n",
    "           states = self.lstm.get_initial_state(x)\n",
    "        x, h_states, c_states = self.lstm(x, initial_state=states, training=training)\n",
    "        # Predict the next tokens and apply log-softmax activation\n",
    "        x = self.dense(x, training=training)\n",
    "        if return_state:\n",
    "            return x, h_states\n",
    "        else:\n",
    "            return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1114f0bc-9f62-4992-bb1f-4318ca70eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = 56\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN layers\n",
    "rnn_units = 512\n",
    "\n",
    "model = LSTMLM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units = rnn_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf86c76-3858-4cc3-b915-07e2452a8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Build the model for inspection\n",
    "model.build(input_shape=(batch_size, sequence_length))\n",
    "model.summary()\n",
    "\n",
    "# Optional: Call the model to ensure it works with the given input shape\n",
    "dummy_input = tf.keras.Input(shape=(sequence_length,))\n",
    "model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "5e5f6ddc-1c21-4b45-8a44-75cb23ae4234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 01:32:56.510125: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-16 01:32:56.511550: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-16 01:32:56.512794: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstmlm_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 100, 256)          14336     \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               [(None, 100, 512),        1574912   \n",
      "                              (None, 512),                       \n",
      "                              (None, 512)]                       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 100, 56)           28728     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,617,976\n",
      "Trainable params: 1,617,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 01:32:56.720045: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-16 01:32:56.721408: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-16 01:32:56.722447: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# testing your model\n",
    "try:\n",
    "    # Simulate inputs of length 100. This allows to compute the shape of all inputs and outputs of our network\n",
    "    model.build(input_shape=(BATCH_SIZE, 100))\n",
    "    model.call(Input(shape=(100)))\n",
    "    model.summary() \n",
    "except:\n",
    "    print(\"\\033[91mError! \\033[0mA problem occurred while building your model. This error can occur due to wrong initialization of the return_sequences parameter\\n\\n\")\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "4fbea812-2e86-4f9d-b1fa-ccaa85d93d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 01:30:29.098165: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-16 01:30:29.099853: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-16 01:30:29.101117: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-07-16 01:30:29.292981: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-16 01:30:29.294091: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-16 01:30:29.295514: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"grulm_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 100, 256)          14336     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 [(None, 100, 512),        1182720   \n",
      "                              (None, 512)]                       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100, 56)           28728     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,225,784\n",
      "Trainable params: 1,225,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GRULM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A GRU-based language model that maps from a tensor of tokens to activations over a vocabulary.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
    "        embedding_dim (int, optional): Depth of embedding. Defaults to 256.\n",
    "        rnn_units (int, optional): Number of units in the GRU cell. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A GRULM language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=256, embedding_dim=256, rnn_units=128):\n",
    "        super().__init__(self)\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Create an embedding layer to map token indices to embedding vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # Define a GRU (Gated Recurrent Unit) layer for sequence modeling\n",
    "        self.gru = tf.keras.layers.GRU(units=rnn_units, return_sequences=True, return_state=True)\n",
    "        # Apply a dense layer with log-softmax activation to predict next tokens\n",
    "        self.dense = tf.keras.layers.Dense(units=vocab_size, activation=tf.nn.log_softmax)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        # Map input tokens to embedding vectors\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            # Get initial state from the GRU layer\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        # Predict the next tokens and apply log-softmax activation\n",
    "        x = self.dense(x, training=training)\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = 56\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN layers\n",
    "rnn_units = 512\n",
    "\n",
    "model = GRULM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units = rnn_units)\n",
    "\n",
    "\n",
    "# testing your model\n",
    "\n",
    "try:\n",
    "    # Simulate inputs of length 100. This allows to compute the shape of all inputs and outputs of our network\n",
    "    model.build(input_shape=(BATCH_SIZE, 100))\n",
    "    model.call(Input(shape=(100)))\n",
    "    model.summary() \n",
    "except:\n",
    "    print(\"\\033[91mError! \\033[0mA problem occurred while building your model. This error can occur due to wrong initialization of the return_sequences parameter\\n\\n\")\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bda6b8-d760-453e-b43b-5ed32cf04ac5",
   "metadata": {},
   "source": [
    "why\n",
    "x, h_states, c_states = self.lstm(x, initial_state=states, training=training)\n",
    "while\n",
    "x, states = self.gru(x, initial_state=states, training=training)\n",
    "?\n",
    "\n",
    "when using the GRU (self.gru) in your GRULM model, you unpack the outputs into x and states because the GRU only returns the hidden state. On the other hand, with the LSTM (self.lstm) in your LSTMLM model, you unpack the outputs into x, h_states, and c_states to capture both the hidden state and the cell state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ca6f4-9882-4fae-ac76-eeb938f51ada",
   "metadata": {},
   "source": [
    "### Prefdiction before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4ed5c9ec-6db6-4c14-a6c4-6da37905a9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 23:49:31.316950: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [5107435]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-07-15 23:49:31.317297: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [5107435]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [32 37 47 48 29 34 34 25  4 29 42 32  4 37  3 36 43 44 33  4 48 43  4 47\n",
      " 33 33  4 29  4 36 43 49 47 33 51 37 34 33  4 48 29 39 33  4 48 36 33 33\n",
      "  4 30 33 48 51 33 33 42  4 36 33 46  4 40 33 35 47  3 29 42 32  4 47 44\n",
      " 37 42  4 37 48  4 43 34 34 13  3 47 37 46  4 29 42 32 46 33 51  2 34 29\n",
      " 37 48 36 11]\n",
      "Target:  [37 47 48 29 34 34 25  4 29 42 32  4 37  3 36 43 44 33  4 48 43  4 47 33\n",
      " 33  4 29  4 36 43 49 47 33 51 37 34 33  4 48 29 39 33  4 48 36 33 33  4\n",
      " 30 33 48 51 33 33 42  4 36 33 46  4 40 33 35 47  3 29 42 32  4 47 44 37\n",
      " 42  4 37 48  4 43 34 34 13  3 47 37 46  4 29 42 32 46 33 51  2 34 29 37\n",
      " 48 36 11  4]\n",
      "(1, 100, 56) # (batch_size, sequence_length, vocab_size)\n",
      "\n",
      " Predictions:  tf.Tensor(\n",
      "[[25 25 43 37 51 22 32 10 10 10 10 12 16 25 16 20 22 18 32 20 20 30 20 16\n",
      "  32 32 52 53 16 20 18 32 32 32 24 52 32 32 52 20 53 51  1 45 51 20 32 32\n",
      "  52 54 54 20 24 32 32 54 20 20 32 54 16 10 54 33 21 32 29 20 54 16 51 13\n",
      "  43  6 10 16 37 37 37 22 32 24 32 32 43 55 39 51 54 54 54 54 54 54 32 29\n",
      "  43 37 20 22]], shape=(1, 100), dtype=int64)\n",
      "\n",
      " [-4.014883  -4.018631  -4.03094   -4.0376368 -4.023419  -4.040088\n",
      " -4.0387754 -4.0174217 -4.026433  -4.0601907 -4.0141287 -4.032281\n",
      " -4.041744  -4.01505   -4.014946  -4.035887  -4.008339  -4.018528\n",
      " -4.005833  -4.03648   -4.0271916 -4.049814  -3.9974117 -4.0313435\n",
      " -4.015154  -4.0247483 -4.028767  -4.0233374 -4.030962  -4.0377045\n",
      " -4.0215816 -4.021785  -4.000556  -4.031686  -4.0325603 -4.034\n",
      " -4.0241866 -4.0044503 -4.0284896 -4.0360765 -4.0318046 -4.0142817\n",
      " -4.0322676 -4.019824  -4.0076084 -4.0390544 -4.0307183 -4.009277\n",
      " -4.010934  -4.041613  -4.017896  -4.027094  -4.0218163 -4.021162\n",
      " -4.0295167 -4.0356064]\n",
      "(56,)\n",
      "\n",
      " tf.Tensor(\n",
      "[[[-4.024893  -4.0351434 -4.029255  ... -4.0313864 -4.017729  -4.033844 ]\n",
      "  [-4.0460205 -4.0217395 -4.0307984 ... -4.035839  -4.0360765 -4.0332136]\n",
      "  [-4.0462775 -4.0255775 -4.024814  ... -4.041131  -4.032236  -4.0346103]\n",
      "  ...\n",
      "  [-4.023337  -4.0234895 -4.013184  ... -4.0130787 -4.03709   -4.0525537]\n",
      "  [-4.008123  -4.017262  -4.021405  ... -4.0246463 -4.0347447 -4.03361  ]\n",
      "  [-4.014883  -4.018631  -4.03094   ... -4.021162  -4.0295167 -4.0356064]]], shape=(1, 100, 56), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input:  [43  4 32 51 29 46 34 37 47 36  4 29 42 32  4 47 43  4 40 43 51 26  3 36\n",
      " 43 51  4 40 43 51  4 29 41  4 37 11  4 48 36 43 49  4 44 29 37 42 48 33\n",
      " 32  4 41 29 53 44 43 40 33 26  4 47 44 33 29 39 25  3 36 43 51  4 40 43\n",
      " 51  4 29 41  4 37 26  4 37  4 29 41  4 42 43 48  4 53 33 48  4 47 43  4\n",
      " 40 43 51  3]\n",
      "Target:  [ 4 32 51 29 46 34 37 47 36  4 29 42 32  4 47 43  4 40 43 51 26  3 36 43\n",
      " 51  4 40 43 51  4 29 41  4 37 11  4 48 36 43 49  4 44 29 37 42 48 33 32\n",
      "  4 41 29 53 44 43 40 33 26  4 47 44 33 29 39 25  3 36 43 51  4 40 43 51\n",
      "  4 29 41  4 37 26  4 37  4 29 41  4 42 43 48  4 53 33 48  4 47 43  4 40\n",
      " 43 51  3 30]\n",
      "(1, 100, 56) # (batch_size, sequence_length, vocab_size)\n",
      "\n",
      " Predictions:  tf.Tensor(\n",
      "[[36 16 16 54 11 54 22 49 43 22 20 10 10 54 16 16 36 16 10 30 24 43 45 20\n",
      "  18 24  0 10 30 24 10 53 51 10 52 16 16 37 20 22 30 20 35 29 34  6 37 32\n",
      "  32 16 51 51 50 35 18 13 32 32 45 20 20 18 29  1  1  8 20 30 24  0 10 30\n",
      "  24 10 53 51 10 52 43 45 43 16 16 51 51 10 36 37 20 50 32 33 20 51 36 16\n",
      "  10 30 24 23]], shape=(1, 100), dtype=int64)\n",
      "\n",
      " [-4.005616  -4.0471067 -4.025796  -4.0265126 -4.0243187 -4.0199533\n",
      " -4.039676  -4.037188  -4.00349   -4.0113735 -4.010734  -4.00636\n",
      " -4.035801  -4.0087647 -4.024445  -4.0241218 -4.0186973 -4.029918\n",
      " -4.042847  -4.0437393 -4.020719  -4.007928  -4.0156784 -4.002673\n",
      " -4.00315   -4.0390096 -4.0253406 -4.040748  -4.0238957 -4.0213017\n",
      " -4.0157046 -4.0269966 -4.0029607 -4.055024  -4.0439916 -4.0363097\n",
      " -4.032968  -4.0443993 -4.0543594 -4.0330725 -4.0348587 -4.0159016\n",
      " -4.020802  -4.01287   -4.0215874 -4.0098987 -4.017458  -4.0341153\n",
      " -4.033396  -4.0186024 -4.0268297 -4.0414767 -4.0340214 -4.0268364\n",
      " -4.0329413 -4.0105114]\n",
      "(56,)\n",
      "\n",
      " tf.Tensor(\n",
      "[[[-4.026344  -4.0242996 -4.0405726 ... -4.0392103 -4.0321455 -4.014091 ]\n",
      "  [-4.0145783 -4.0304976 -4.0285797 ... -4.0269547 -4.0300007 -4.019681 ]\n",
      "  [-4.0171814 -4.038836  -4.0309443 ... -4.031031  -4.0196247 -4.0314693]\n",
      "  ...\n",
      "  [-4.0237007 -4.03768   -4.051492  ... -4.0490737 -4.0234213 -4.0211186]\n",
      "  [-4.009516  -4.029941  -4.038872  ... -4.0322466 -4.005926  -4.007888 ]\n",
      "  [-4.005616  -4.0471067 -4.025796  ... -4.0268364 -4.0329413 -4.0105114]]], shape=(1, 100, 56), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(2):\n",
    "    print(\"Input: \", input_example_batch[0].numpy()) # Lets use only the first sequence on the batch\n",
    "    print(\"Target: \", target_example_batch[0].numpy())\n",
    "    example_batch_predictions = model(tf.constant([input_example_batch[0].numpy()]))\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    print(\"\\n Predictions: \" , tf.argmax(example_batch_predictions, axis=-1))\n",
    "    \n",
    "    print(\"\\n\" , example_batch_predictions[0][99].numpy())\n",
    "    print(example_batch_predictions[0][99].numpy().shape)\n",
    "    print(\"\\n\" , example_batch_predictions)\n",
    "    \n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc2b56-fa7e-472b-a063-62095e254dec",
   "metadata": {},
   "source": [
    "**Predictions is a tensor of log probabilities**              \n",
    "The tf.nn.log_softmax function computes the logarithm of the softmax activation along the last dimension of the tensor. Softmax itself ensures that the outputs are probabilities (values between 0 and 1), but applying log_softmax afterwards converts these probabilities into log-probabilities, which can be negative.\n",
    "\n",
    "Here's why log-probabilities (negative values) are commonly used:\n",
    "Numerical Stability: When dealing with probabilities in machine learning models, it's often more numerically stable to work with log-probabilities, especially during computation of gradients and when dealing with small probabilities.\n",
    "Loss Calculation: Many loss functions, like cross-entropy, expect log-probabilities as inputs because they penalize the distance between predicted and actual probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b35eadea-d370-4b68-8c84-36b16f0358ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "b'h'\n"
     ]
    }
   ],
   "source": [
    "# Prediction for the last character in a sequence\n",
    "last_character = tf.math.argmax(example_batch_predictions[0][99]).numpy()\n",
    "print(last_character)\n",
    "print(text_from_ids([last_character] , vocab).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8dd14766-fadb-4a8c-b0e2-3f57079752a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[36 16 16 54 11 54 22 49 43 22 20 10 10 54 16 16 36 16 10 30 24 43 45 20\n",
      "  18 24  0 10 30 24 10 53 51 10 52 16 16 37 20 22 30 20 35 29 34  6 37 32\n",
      "  32 16 51 51 50 35 18 13 32 32 45 20 20 18 29  1  1  8 20 30 24  0 10 30\n",
      "  24 10 53 51 10 52 43 45 43 16 16 51 51 10 36 37 20 50 32 33 20 51 36 16\n",
      "  10 30 24 23]], shape=(1, 100), dtype=int64)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "predictions = tf.argmax(example_batch_predictions, axis=-1)\n",
    "print(predictions)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "a76a20ab-644b-4354-b334-7fb57b16c05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b\"h22z,z8uo86))z22h2)b:oq64:[UNK])b:)yw)x22i68b6gaf$idd2wwvg4.ddq664a'6b:[UNK])b:)yw)xoqo22ww)hi6vde6wh2)b:9\"]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text_from_ids(predictions , vocab).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "0254f430-2d8c-4ffb-a903-d56ec8bb82c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '', '\\t', '\\n', ' ', '!', '$', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22f55a-1ef5-4a9e-acf6-7738d6e442e4",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 5) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "404507af-7488-4914-bcf8-52033bc01ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00125)\n",
    "model.compile(optimizer=optimizer , loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "263ffa95-0846-475c-ae66-8dd5652ce421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 01:33:24.988258: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-16 01:33:24.989877: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-16 01:33:24.991627: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-07-16 01:33:25.627747: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-07-16 01:33:25.629750: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-07-16 01:33:25.631742: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791/791 [==============================] - 675s 848ms/step - loss: 1.9774\n",
      "Epoch 2/10\n",
      "791/791 [==============================] - 1012s 1s/step - loss: 1.5063\n",
      "Epoch 3/10\n",
      "791/791 [==============================] - 991s 1s/step - loss: 1.3955\n",
      "Epoch 4/10\n",
      "791/791 [==============================] - 1333s 2s/step - loss: 1.3424\n",
      "Epoch 5/10\n",
      "791/791 [==============================] - 1093s 1s/step - loss: 1.3091\n",
      "Epoch 6/10\n",
      "791/791 [==============================] - 1869s 2s/step - loss: 1.2848\n",
      "Epoch 7/10\n",
      "791/791 [==============================] - 1163s 1s/step - loss: 1.2661\n",
      "Epoch 8/10\n",
      "791/791 [==============================] - 1655s 2s/step - loss: 1.2513\n",
      "Epoch 9/10\n",
      "791/791 [==============================] - 1237s 2s/step - loss: 1.2380\n",
      "Epoch 10/10\n",
      "791/791 [==============================] - 569s 717ms/step - loss: 1.2271\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "9a4e108a-70f4-4961-9fb9-fd8f955fee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstmlm_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 100, 256)          14336     \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               [(None, 100, 512),        1574912   \n",
      "                              (None, 512),                       \n",
      "                              (None, 512)]                       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 100, 56)           28728     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,617,976\n",
      "Trainable params: 1,617,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7f7ba4ab-1440-4e9f-8e9e-da54fbdc35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('/mnt/market/anclab-rstudio-server/home/mpir0002/NLP_labs/lstm_model_Shekespeare.h5')\n",
    "#os.chdir('/mnt/market/anclab-rstudio-server/home/mpir0002/NLP_labs/')\n",
    "#model.save('lstm_model_Shekespeare', save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3fbabd28-d791-407c-8eac-06c1853de63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model architecture to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"/mnt/market/anclab-rstudio-server/home/mpir0002/NLP_labs/lstm_model_Shekespeare.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save weights to HDF5\n",
    "model.save_weights(\"/mnt/market/anclab-rstudio-server/home/mpir0002/NLP_labs/lstm_model_Shekespeare_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ecef1-f25e-4658-a79b-1901aa846be9",
   "metadata": {},
   "source": [
    "### Prefdictions after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7d2fa2d4-69a6-4d1e-aee5-455b9192262a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 05:53:29.512833: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [5107435]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-07-16 05:53:29.513275: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int64 and shape [5107435]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [44 44 33 29 46 47  4 29  4 34 29 31 33  3 48 36 29 48  4 43 50 33 46 12\n",
      " 35 43 33 47  4 41 53  4 30 40 49 42 48  4 37 42 50 33 42 48 37 43 42  4\n",
      " 45 49 37 48 33 11  3 32 49 40 40 37 42 35  4 41 53  4 40 37 42 33 47  4\n",
      " 29 42 32  4 32 43 37 42 35  4 41 33  4 32 37 47 35 46 29 31 33 13  3 51\n",
      " 33 46 33  4]\n",
      "Target:  [44 33 29 46 47  4 29  4 34 29 31 33  3 48 36 29 48  4 43 50 33 46 12 35\n",
      " 43 33 47  4 41 53  4 30 40 49 42 48  4 37 42 50 33 42 48 37 43 42  4 45\n",
      " 49 37 48 33 11  3 32 49 40 40 37 42 35  4 41 53  4 40 37 42 33 47  4 29\n",
      " 42 32  4 32 43 37 42 35  4 41 33  4 32 37 47 35 46 29 31 33 13  3 51 33\n",
      " 46 33  4 37]\n",
      "\n",
      " Predictions shape (1, 100, 56) # (batch_size, sequence_length, vocab_size)\n",
      "Predictions:  tf.Tensor(\n",
      "[[46 33 29 46 33  4 48 42 47 29 37 33  4 43 36 29 48  4 51 42 33 46 48 46\n",
      "  33 43 47  4 48 53  4 40 46 43 47 48  4 48 42  4 37 42 48 37 43 42  4 48\n",
      "  49 37 48 33  4  3 29 33 46 40  4 42 35  4 48 33  4 34 43 34 33  4  4 29\n",
      "  42 32  4 47 37 49 42 35  4 48 43 42 48 33 47 31 46 29 31 33 32  3 27 36\n",
      "  40 33  4 37]], shape=(1, 100), dtype=int64)\n",
      "\n",
      " [-21.241055  -20.741137  -17.613518  -19.863024  -10.795312  -20.355669\n",
      " -20.312107  -14.261922   -5.949542  -17.610292  -24.066101  -18.650347\n",
      " -13.400929  -22.359938  -20.668894  -18.224499  -18.403337  -19.417585\n",
      " -17.581034  -16.782932  -19.000639  -19.54839   -17.896887  -19.608326\n",
      " -21.309427  -21.221012  -22.719564  -13.319862  -20.46582    -3.0531747\n",
      "  -4.5253253  -3.5461957  -5.033404   -3.8422816  -4.5769253  -4.4065256\n",
      "  -2.8970642  -1.192292   -6.279599   -5.887292   -4.975444   -2.8390417\n",
      "  -2.0592568  -4.0015845  -4.055957   -7.5514517  -5.666049   -2.9247675\n",
      "  -2.1354954  -6.0716867  -5.6175823  -3.7303681 -11.045072   -2.7749429\n",
      " -12.087224  -17.506475 ]\n",
      "(56,)\n",
      "\n",
      " tf.Tensor(\n",
      "[[[-13.032911  -12.540045   -7.8715987 ...  -5.4231405 -12.001658\n",
      "   -14.812551 ]\n",
      "  [-16.731087  -16.488634  -11.217572  ...  -3.61228   -16.668278\n",
      "   -17.995783 ]\n",
      "  [-15.8697    -15.91534   -10.431948  ...  -4.468832   -9.108452\n",
      "   -18.914835 ]\n",
      "  ...\n",
      "  [-22.308466  -20.866047  -15.689557  ...  -8.568362  -17.17048\n",
      "   -20.843454 ]\n",
      "  [-25.205055  -24.509094  -11.579229  ... -14.357103  -20.266212\n",
      "   -21.252457 ]\n",
      "  [-21.241055  -20.741137  -17.613518  ...  -2.7749429 -12.087224\n",
      "   -17.506475 ]]], shape=(1, 100, 56), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input:  [33 13  3  8 34 29 37 46 11  4 39 37 42 32  4 29 42 32  4 48 46 49 33  8\n",
      "  4 37 47  4 29 40 40  4 41 53  4 29 46 35 49 41 33 42 48 11  3  8 34 29\n",
      " 37 46 11  4 39 37 42 32 11  4 29 42 32  4 48 46 49 33  8  4 50 29 46 53\n",
      " 37 42 35  4 48 43  4 43 48 36 33 46  4 51 43 46 32 47 25  3 29 42 32  4\n",
      " 37 42  4 48]\n",
      "Target:  [13  3  8 34 29 37 46 11  4 39 37 42 32  4 29 42 32  4 48 46 49 33  8  4\n",
      " 37 47  4 29 40 40  4 41 53  4 29 46 35 49 41 33 42 48 11  3  8 34 29 37\n",
      " 46 11  4 39 37 42 32 11  4 29 42 32  4 48 46 49 33  8  4 50 29 46 53 37\n",
      " 42 35  4 48 43  4 43 48 36 33 46  4 51 43 46 32 47 25  3 29 42 32  4 37\n",
      " 42  4 48 36]\n",
      "\n",
      " Predictions shape (1, 100, 56) # (batch_size, sequence_length, vocab_size)\n",
      "Predictions:  tf.Tensor(\n",
      "[[ 4  3 27 48 29 37 46  4  4 37 37 42 35  4 43 42 32  4 31 36 49 33  4 47\n",
      "  48 42  4 48  4 40  4 48 53  4 36 46 41 49 41 33 42 48 11  3 48 48 29 37\n",
      "  48  4  4 37 37 42 35  4  4 48 42 32  4 48 36 49 33  4 47 48 37 42 37 37\n",
      "  42 35  4 48 36  4 48 49 36 33 46  4 48 37 46 32 47 13  3 29 42 32  4 48\n",
      "   4  4 48 36]], shape=(1, 100), dtype=int64)\n",
      "\n",
      " [-2.16920357e+01 -2.19636250e+01 -2.16484585e+01 -1.88450451e+01\n",
      " -1.43288965e+01 -2.04905033e+01 -2.21908398e+01 -2.07067986e+01\n",
      " -1.36074572e+01 -1.99670429e+01 -2.53973198e+01 -1.72287598e+01\n",
      " -1.71730938e+01 -1.93241444e+01 -2.03319759e+01 -2.15927887e+01\n",
      " -2.28343506e+01 -2.03742466e+01 -2.16025982e+01 -2.10129108e+01\n",
      " -2.22928658e+01 -2.18029137e+01 -2.18310452e+01 -2.24961319e+01\n",
      " -2.01205521e+01 -1.95996666e+01 -2.14687271e+01 -2.08419952e+01\n",
      " -2.14152966e+01 -8.13620758e+00 -1.77300014e+01 -1.41494284e+01\n",
      " -2.29233990e+01 -7.43014669e+00 -1.91134930e+01 -1.57599249e+01\n",
      " -7.14615593e-03 -7.20744944e+00 -1.61771011e+01 -1.76799908e+01\n",
      " -1.35719938e+01 -1.49661989e+01 -1.41255350e+01 -6.12012148e+00\n",
      " -1.81706600e+01 -1.66180763e+01 -6.66655493e+00 -1.74834881e+01\n",
      " -1.46454697e+01 -8.31452847e+00 -2.03898048e+01 -7.30967951e+00\n",
      " -1.51066999e+01 -6.80986834e+00 -1.79079323e+01 -2.05249958e+01]\n",
      "(56,)\n",
      "\n",
      " tf.Tensor(\n",
      "[[[-12.084672  -11.816034   -5.327223  ...  -4.592429   -7.696761\n",
      "   -12.277061 ]\n",
      "  [-21.224031  -21.196789   -8.583444  ... -23.51399   -18.716715\n",
      "   -18.285025 ]\n",
      "  [-15.347696  -14.369797  -12.800219  ...  -4.928359  -12.558811\n",
      "    -8.41393  ]\n",
      "  ...\n",
      "  [-19.463234  -19.32743   -10.410703  ... -11.023468  -14.840334\n",
      "   -15.611113 ]\n",
      "  [-20.524998  -20.334753  -18.905958  ...  -4.344208  -10.919809\n",
      "   -16.785345 ]\n",
      "  [-21.692036  -21.963625  -21.648458  ...  -6.8098683 -17.907932\n",
      "   -20.524996 ]]], shape=(1, 100, 56), dtype=float32)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(2):\n",
    "    print(\"Input: \", input_example_batch[0].numpy()) # Lets use only the first sequence on the batch\n",
    "    print(\"Target: \", target_example_batch[0].numpy())\n",
    "    example_batch_predictions = model(tf.constant([input_example_batch[0].numpy()]))\n",
    "    print(\"\\n Predictions shape\", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    print(\"Predictions: \" , tf.argmax(example_batch_predictions, axis=-1))\n",
    "    \n",
    "    print(\"\\n\" , example_batch_predictions[0][99].numpy())\n",
    "    print(example_batch_predictions[0][99].numpy().shape)\n",
    "    print(\"\\n\" , example_batch_predictions)\n",
    "    \n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13376110-c7af-4a11-82a8-57e0197cacf0",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 6) Evaluating model using log perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae7c75-99b6-489c-aa71-b34549067287",
   "metadata": {},
   "source": [
    "In the following, we will write a program takes in preds and target. preds is a tensor of log probabilities. You can use tf.one_hot to transform the target into the same dimension. You then multiply them and sum them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "95932590-f230-45c3-a386-307b82ee6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_perplexity(preds, target):\n",
    "    \"\"\"\n",
    "    Function to calculate the log perplexity of a model.\n",
    "\n",
    "    Args:\n",
    "        preds (tf.Tensor): Predictions of a list of batches of tensors corresponding to lines of text.\n",
    "        target (tf.Tensor): Actual list of batches of tensors corresponding to lines of text.\n",
    "\n",
    "    Returns:\n",
    "        float: The log perplexity of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    PADDING_ID = 1\n",
    "    \n",
    "    target_one_hot = tf.one_hot(target, depth=preds.shape[-1])\n",
    "    \n",
    "    log_p = np.sum(target_one_hot * preds.numpy(), axis= -1)   # Identify non-padding elements in the target\n",
    "    \n",
    "    non_pad = 1.0 - np.equal(target, PADDING_ID)   # The line is used to create a mask that identifies non-padding elements in the target tensor.\n",
    "    \n",
    "    log_p = log_p * non_pad       # Apply non-padding mask to log probabilities to exclude padding\n",
    "    \n",
    "    log_ppx = np.sum(log_p, axis=-1) / np.sum(non_pad, axis=-1) # Calculate the log perplexity by taking the sum of log probabilities and dividing by the sum of non-padding elements\n",
    "    \n",
    "    log_ppx = np.mean(log_ppx) # Compute the mean of the previous expression\n",
    "        \n",
    "    return -log_ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "c4c260ff-f800-4d68-a6e5-12c3b24af986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c8585705-45cd-4e0d-91c1-65a07fb4b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log perplexity and perplexity of your model are 1.2003743380375573 and 3.321360001441171 respectively\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_text = \"\\n\".join(eval_lines)\n",
    "eval_ids = line_to_tensor(eval_text, vocab)\n",
    "input_ids, target_ids = split_input_target(eval_ids)\n",
    "\n",
    "preds, status = model(tf.expand_dims(input_ids, 0), training=False, states=None, return_state=True)\n",
    "\n",
    "#Get the log perplexity\n",
    "log_ppx = log_perplexity(preds, tf.expand_dims(target_ids, 0))\n",
    "print(f'The log perplexity and perplexity of your model are {log_ppx} and {np.exp(log_ppx)} respectively')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6dde06eb-7d53-4114-9f18-49d2ad436b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 41573), dtype=int64, numpy=array([[27, 33, 52, ..., 49, 42, 48]])>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(input_ids, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "c4c723f5-7e79-4e7b-b7f7-613ba1fa1bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512)\n",
      "(1, 41573, 56)\n"
     ]
    }
   ],
   "source": [
    "print(status.shape) # Last hidden state dimension\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51323695-01f9-474c-bbf9-3cd1ad0cbf1d",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 7) Generative language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d21c5a-f07c-49a0-b78b-b1a6d71d7b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1e571-741c-4e4a-9276-b32d4190eec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524a0f9-9ad9-4c94-812b-7e5a2196633a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ded5a0-caee-4ac6-a17d-5867803cbc02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13702a5f-686d-4c5c-b13d-aefc481a82a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276fcec-3d6b-4f45-8699-18f515357dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea3da7-b99d-49af-ae54-850dc6671177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
